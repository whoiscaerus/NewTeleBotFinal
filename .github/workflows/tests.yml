name: CI/CD Tests

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
      - develop

jobs:
  # Check if we should skip CI based on commit message
  check-skip:
    runs-on: ubuntu-latest
    outputs:
      skip: ${{ steps.check.outputs.skip }}
    steps:
      - name: Check for skip markers
        id: check
        run: |
          # Use a grep-based check for skip markers to avoid bash [[ =~ ]] regex portability issues
          COMMIT_MSG="${{ github.event.head_commit.message }}"
          echo "Commit message: $COMMIT_MSG"
          if echo "$COMMIT_MSG" | grep -E '\[skip-ci\]|\[ci-skip\]|\[skip[[:space:]]ci\]|\[ci[[:space:]]skip\]' >/dev/null 2>&1; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è CI will be skipped (found skip marker in commit message)"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
            echo "‚úÖ CI will run normally"
          fi

  lint:
    needs: check-skip
    if: needs.check-skip.outputs.skip != 'true'
    name: Lint Code
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e ".[dev]"

      - name: Run Black formatter check
        run: python -m black --check --diff backend/

      - name: Run Ruff linter
        run: python -m ruff check backend/

      - name: Run isort import check
        run: python -m isort --check-only backend/

  typecheck:
    needs: check-skip
    if: needs.check-skip.outputs.skip != 'true'
    name: Type Checking
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e ".[dev]"

      - name: Run mypy type checker
        working-directory: ./backend
        run: python -m mypy app --config-file=../mypy.ini

  tests:
    needs: check-skip
    if: needs.check-skip.outputs.skip != 'true'
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11"]

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: trading_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e ".[dev]"

      - name: Wait for services
        run: |
          # Wait for PostgreSQL to be ready
          for i in {1..30}; do
            if python -c "import psycopg; psycopg.connect('postgresql+psycopg://postgres:postgres@localhost:5432/trading_db')" 2>/dev/null; then
              echo "PostgreSQL is ready"
              break
            fi
            echo "Waiting for PostgreSQL... ($i/30)"
            sleep 2
          done

      - name: Run database migrations
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/trading_db
        run: |
          # SKIP: Alembic migration chain is broken (13 unresolved dependencies)
          # Tests will create tables via SQLAlchemy fixtures instead
          echo "Skipping alembic migrations - using test fixtures for schema"

      - name: Collect tests diagnostic (CI environment - DETAILED)
        env:
          CI: "true"
          DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/trading_db
          REDIS_URL: redis://localhost:6379/0
          APP_ENV: test
        run: |
          echo "=========================================="
          echo "COMPREHENSIVE TEST COLLECTION DIAGNOSTIC"
          echo "=========================================="

          echo ""
          echo "1. Test collection with verbose output:"
          python -m pytest backend/tests --collect-only 2>&1 | head -100

          echo ""
          echo "2. Total test count:"
          TEST_COUNT=$(python -m pytest backend/tests --collect-only -q 2>&1 | tail -1)
          echo "Result: $TEST_COUNT"

          echo ""
          echo "3. Tests by directory:"
          find backend/tests -name "test_*.py" -type f | wc -l

          echo ""
          echo "4. Collection with errors (if any):"
          python -m pytest backend/tests --collect-only -q 2>&1 | grep -E "ERROR|error|Error" || echo "No collection errors found"

          echo ""
          echo "5. Collection with warnings/import failures:"
          python -m pytest backend/tests --collect-only 2>&1 | grep -E "ImportError|ModuleNotFoundError|SyntaxError" | head -20 || echo "No import errors"

          echo ""
          echo "6. Full collection output saved to ci_collected_tests.txt"
          python -m pytest backend/tests --collect-only -q 2>&1 | tee ci_collected_tests.txt

      - name: Run pytest with coverage (Backend) - FULL TEST SUITE
        timeout-minutes: 120
        env:
          CI: "true"
          DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/trading_db
          REDIS_URL: redis://localhost:6379/0
          APP_ENV: test
          APP_LOG_LEVEL: INFO
          PYTHONFAULTHANDLER: "1"
        run: |
          echo "=========================================="
          echo "RUNNING FULL TEST SUITE - ALL 6400+ TESTS"
          echo "=========================================="
          echo "Note: Verbose output saved to file (console limited to summary)"
          echo ""

          # Ensure output directories exist
          mkdir -p coverage/backend
          mkdir -p test-results

          # Run pytest WITHOUT -v to reduce console output
          # Use -q for quiet mode, --tb=short for concise traceback
          # Add --timeout to kill any individual tests that hang (60s per test)
          # CRITICAL: Use absolute path for JSON report to ensure it's created
          set +e  # Don't exit on test failures
          python -m pytest backend/tests \
            --cov=backend/app \
            --cov-report=xml:coverage/backend/coverage.xml \
            --cov-report=html:coverage/backend/htmlcov \
            --cov-report=term-missing:skip-covered \
            --json-report \
            --json-report-file=./test-results/test_results.json \
            --json-report-indent=2 \
            --tb=short \
            --timeout=60 \
            --timeout-method=thread \
            -q 2>&1 | tee test-results/test_output.log
          TEST_EXIT_CODE=$?
          set -e  # Re-enable exit on error

          echo ""
          echo "‚úÖ Test run completed with exit code: $TEST_EXIT_CODE"
          echo ""
          echo "=========================================="
          echo "TEST RUN SUMMARY (Last 200 lines)"
          echo "=========================================="
          tail -200 test-results/test_output.log || echo "(log file not found)"

          echo ""
          echo "=========================================="
          echo "FILES CREATED FOR ARTIFACTS"
          echo "=========================================="
          ls -lh test-results/ || echo "test-results directory not found"
          ls -lh coverage/backend/*.xml || echo "coverage XML not found"

          echo ""
          echo "=========================================="
          echo "JSON REPORT STATUS"
          echo "=========================================="
          if [ -f "test-results/test_results.json" ]; then
            echo "‚úÖ JSON report created successfully"
            echo "Size: $(du -h test-results/test_results.json | cut -f1)"
            echo "Tests in report: $(python -c 'import json; print(len(json.load(open("test-results/test_results.json")).get("tests", [])))' 2>/dev/null || echo 'unknown')"
          else
            echo "‚ùå JSON report NOT FOUND - this will cause report generation to fail"
            echo "Checking for JSON report in other locations:"
            find . -name "test_results.json" -o -name "*.json" -path "*/test-results/*" 2>/dev/null || echo "No JSON files found"
          fi

          echo ""
          echo "=========================================="
          echo "FULL OUTPUT SAVED TO ARTIFACTS"
          echo "=========================================="

          # Exit with original test exit code
          exit $TEST_EXIT_CODE

      - name: Generate comprehensive test results report
        if: always()
        run: |
          echo "=========================================="
          echo "GENERATING COMPREHENSIVE TEST RESULTS REPORT"
          echo "=========================================="

          # Ensure output directory exists
          mkdir -p test-results

          # Check if JSON report exists
          if [ -f "test-results/test_results.json" ]; then
            echo "‚úÖ JSON report found, generating comprehensive report..."
            python scripts/generate_test_report.py \
              --json test-results/test_results.json \
              --output test-results/TEST_RESULTS_REPORT.md \
              --csv test-results/TEST_FAILURES.csv

            if [ $? -eq 0 ]; then
              echo "‚úÖ Report generation successful"
              echo ""
              echo "Report Summary (first 100 lines):"
              head -100 test-results/TEST_RESULTS_REPORT.md
            else
              echo "‚ö†Ô∏è Report generation script failed"
            fi
          else
            echo "‚ùå JSON report not found at test-results/test_results.json"
            echo "   Creating placeholder comprehensive report..."

            mkdir -p test-results
            cat > test-results/TEST_RESULTS_REPORT.md << 'PLACEHOLDER_EOF'
# üß™ Comprehensive Test Results Report

**Generated**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')

## ‚ö†Ô∏è Report Generation Issue

The pytest JSON report was not found at expected location: `test-results/test_results.json`

This typically means one of the following occurred:
- ‚ùå pytest was terminated due to timeout (>120 minutes)
- ‚ùå pytest crashed or failed to complete normally
- ‚ùå pytest-json-report plugin failed to initialize
- ‚ùå Tests collection phase failed completely
- ‚ùå Disk space or memory issues during test execution

## üìã Available Artifacts

1. **test_output.log** - Full pytest console output (search for errors here)
2. **ci_collected_tests.txt** - List of tests that were collected before failure
3. **coverage/backend/htmlcov/** - HTML coverage report

## üîç Troubleshooting Steps

### Step 1: Check the Test Output Log
```bash
# Download test_output.log from CI artifacts
# Search for the last test that ran
# Look for patterns like:
#   - TIMEOUT
#   - FAILED
#   - ERROR
#   - AssertionError
```

### Step 2: Identify the Problem Test
Once you find where the test run stopped, note the test name and run it locally:

```powershell
# Run the specific failing test
.venv/Scripts/python.exe -m pytest backend/tests/test_file.py::test_name -xvs

# Or run with timeout detection
.venv/Scripts/python.exe -m pytest backend/tests/ --timeout=60 -x
```

### Step 3: Check Common Issues

| Issue | Symptom | Fix |
|-------|---------|-----|
| Memory exhaustion | Last tests very slow, then timeout | Reduce test parallelism or split into smaller batches |
| Database connection pool | "too many connections" errors | Check database max_connections setting |
| Redis/Postgres services | "Connection refused" early in log | Verify CI service containers started correctly |
| Hanging test | Test runs but never completes | Add timeout decorator or investigate external API calls |
| Import error | Collection errors at start | Check Python imports and dependency installation |

### Step 4: Review Logs

Key sections to search in `test_output.log`:
- `FAILED` - Lists all failed tests
- `ERROR` - Collection or setup errors
- `TIMEOUT` - Tests that exceeded time limit
- `PASSED` - Confirms tests that did run successfully

### Step 5: Fix & Re-run

Once you identify and fix the issue:

```bash
# Verify locally first
.venv/Scripts/python.exe -m pytest backend/tests/ -v --tb=short

# Then push to GitHub
git add .
git commit -m "Fix: [describe fix]"
git push origin main
```

## üìä What We Know

- **Timeout**: Individual tests limited to 60 seconds
- **Job Timeout**: CI job limited to 120 minutes
- **Full Output**: Available in `test_output.log` artifact
- **Test Collection**: Results in `ci_collected_tests.txt` artifact

---

*This placeholder was created because pytest JSON report generation failed. The comprehensive report will be available once the test suite completes successfully.*

**Time to Debug**: Usually 5-15 minutes to identify the issue locally

PLACEHOLDER_EOF

            echo "Created placeholder comprehensive report"
          fi

          echo ""
          echo "=========================================="
          echo "Report file(s) created:"
          ls -lh test-results/*.md test-results/*.csv 2>/dev/null || echo "No reports found"
          echo "=========================================="

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            test-results/
            ci_collected_tests.txt
            coverage/backend/
          retention-days: 30
          if-no-files-found: warn

      - name: Display test summary
        if: always()
        run: |
          echo "üìä Test Results Summary:"
          if [ -f TEST_FAILURES_DETAILED.md ]; then
            head -n 50 TEST_FAILURES_DETAILED.md
          fi

      - name: Install Node dependencies (Frontend)
        run: npm ci

      - name: Run Jest with coverage (Frontend)
        run: npm run test:ci
        continue-on-error: true

      - name: Merge coverage reports
        run: |
          mkdir -p coverage/merged
          # Copy both coverage files to merged directory for Codecov
          cp coverage/backend/coverage.xml coverage/merged/backend-coverage.xml 2>/dev/null || true
          cp coverage/frontend/coverage.xml coverage/merged/frontend-coverage.xml 2>/dev/null || true
          # List what we have
          echo "Coverage files generated:"
          find coverage -name "*.xml" -type f

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v4
        with:
          directory: ./coverage
          files: ./coverage/backend/coverage.xml,./coverage/frontend/coverage.xml
          token: ${{ secrets.CODECOV_TOKEN }}
          flags: backend,frontend
          name: codecov-umbrella
          verbose: true
          fail_ci_if_error: false  # Don't fail on frontend coverage if tests haven't been written yet

  security:
    name: Security Checks
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install bandit safety

      - name: Run Bandit security check
        run: python -m bandit -r backend/app -f json -o bandit-report.json
        continue-on-error: true

      - name: Run Safety check
        run: python -m safety check --json
        continue-on-error: true

  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [lint, typecheck, tests, security]
    if: success()

    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Build Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./docker/backend.Dockerfile
          target: production
          push: false
          tags: trading-signal-platform:latest

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [lint, typecheck, tests, security, build]
    if: always()

    steps:
      - name: Check test results
        run: |
          if [ "${{ needs.lint.result }}" = "failure" ] || \
             [ "${{ needs.typecheck.result }}" = "failure" ] || \
             [ "${{ needs.tests.result }}" = "failure" ] || \
             [ "${{ needs.security.result }}" = "failure" ] || \
             [ "${{ needs.build.result }}" = "failure" ]; then
            echo "‚ùå One or more checks failed"
            exit 1
          fi
          echo "‚úÖ All checks passed!"
